{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\Anaconda3\\Lib\\site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# For loading tweets to generators\n",
    "from itertools import chain\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tweets into notebook\n",
    "The name of the game here will be to load tweets with minimum memory usage. My plan is to store the tweets (saved in .json format) in a generator. Then iterate through the object and only store desired information from the tweet. I want to store the following:\n",
    "\n",
    "- text\n",
    "- date created\n",
    "- retweet or original\n",
    "- user\n",
    "- number of likes\n",
    "- number of retweets\n",
    "- number of user followers\n",
    "- number of user following\n",
    "\n",
    "#### Analysis outlook\n",
    "\n",
    "It might be interesting to try and predict things like number of likes based on the last three features. By converting the number of likes into catagories e.g. none (0), low (1-5), moderate(6-15), high(16-100), famous(100+) we are open to a range of machine learning algorithms such as kNN, binary search tree, OvR (one vs. rest) linear models e.g. linear regression and logistic regression. Should choose categories such that each is well represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use glob to search through all dates with specific hashtag\n",
    "# then read the files and load the cumulitive result into generator\n",
    "\n",
    "def merge_tweets(file_root, skip=100, file_start=''):\n",
    "    ''' Function that compiles tweets from multiple files\n",
    "        into a single list.  This may take a while.\n",
    "        \n",
    "        file_root (str) - Root directory to folder\n",
    "        skip (int)      - To save memory, skip over\n",
    "                          files using the rule:\n",
    "                          if i%skip == 0. e.g. skip=1\n",
    "                          reads in all tweets because\n",
    "                          i%1 == 0 for all integers i. '''\n",
    "    \n",
    "    if not file_start:\n",
    "        file_start = file_root\n",
    "    file_root = file_root + '/' + file_start + '*'\n",
    "    tweet_files = list(glob.iglob(file_root))\n",
    "    tweets = iter(())\n",
    "    for f in tweet_files:\n",
    "        t = load_tweets(f, skip)\n",
    "        tweets = chain(tweets, t)\n",
    "    return tweets\n",
    "\n",
    "def load_tweets(file, skip):\n",
    "    with open(file, 'r') as f:\n",
    "        tweets = (json.loads(line) for i, line in enumerate(f.readlines()) if i%skip==0)\n",
    "    return tweets\n",
    "    \n",
    "# Put tweets into a dictionary\n",
    "all_tweets = {}\n",
    "\n",
    "# Input folder names\n",
    "search_phrases = ['test_files']\n",
    "\n",
    "for folder in search_phrases:\n",
    "    all_tweets[folder] = merge_tweets(file_root=folder, skip=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_files': <itertools.chain at 0x206a7df0668>}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate through tweets, check for NHL related\n",
    "# phrases in tweet['text'] and save qualifying\n",
    "# tweets to a new file\n",
    "\n",
    "criteria = {'#nhl': ['nhl'],\n",
    "            'test_files': ['nhl'],\n",
    "            'Pavelski': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                         'SJS', 'sjs', 'sharks', 'Sharks',\n",
    "                         'jose', 'Jose', 'Joe'],\n",
    "            'Lucic': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                      'LAK', 'kings', 'Kings',\n",
    "                      'angeles', 'Angeles', 'Milan'],\n",
    "            'Ovechkin': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                         'WSH', 'wsh', 'caps', 'Caps',\n",
    "                         'capitals', 'Capitals',\n",
    "                         'washington', 'Washington',\n",
    "                         'Alex'],\n",
    "            'Giroux': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                       'PHI', 'phi', 'flyers', 'Flyers',\n",
    "                       'Philadelphia', 'Claude'],\n",
    "            'Jagr': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                     'FLA', 'fla', 'panthers', 'Panthers',\n",
    "                     'florida', 'Florida', 'Jaromir'],\n",
    "            'Tavares': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                        'NYI', 'nyi', 'islanders', 'Islanders',\n",
    "                        'york', 'York', 'John'],\n",
    "            'Kucherov': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                         'TBL', 'tbl', 'lightning', 'Lightning',\n",
    "                         'tampa', 'Tampa', 'Nikita'],\n",
    "            'Mrazek': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                       'DET', 'det', 'Wings', 'wings',\n",
    "                       'Detroit', 'Petr'],\n",
    "            'Seguin': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                       'DAL', 'dal', 'stars', 'Stars',\n",
    "                       'Dallas', 'Tyler'],\n",
    "            'Pominville': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                           'MIN', 'min', 'wild', 'Wild',\n",
    "                           'Minnesota', 'Jason'],\n",
    "            'Crosby': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                       'PIT', 'pit', 'penguins', 'Penguins',\n",
    "                       'Pittsburgh', 'Sidney'],\n",
    "            'Lundqvist': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                          'NYR', 'nyr', 'rangers', 'Rangers',\n",
    "                          'york', 'York', 'Henrik'],\n",
    "            'Tarasenko':['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                         'STL', 'stl', 'blues', 'Blues',\n",
    "                         'louis', 'Louis', 'Vladimir'],\n",
    "            'Kane': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                     'CHI', 'chi', 'hawks', 'Hawks',\n",
    "                     'chicago', 'Chicago', 'Patrick'],\n",
    "            'Perry': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                      'ANA', 'ana', 'ducks', 'Ducks',\n",
    "                      'Anaheim', 'Corey'],\n",
    "            'Forsberg': ['NHL', 'nhl', 'hockey', 'Hockey',\n",
    "                         'NSH', 'nsh', 'predators', 'Predators',\n",
    "                         'Nashville', 'Filip']}\n",
    "\n",
    "if True:\n",
    "    for folder in all_tweets.keys():\n",
    "        with open(folder+'/filtered_tweets.json', 'w') as f:\n",
    "            for t in all_tweets[folder]:\n",
    "                for word in criteria[folder]:\n",
    "                    if word in t['text']:\n",
    "                        json.dump(t, f)\n",
    "                        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in filtered tweets\n",
    "\n",
    "# Put tweets into a dictionary\n",
    "all_tweets = {}\n",
    "\n",
    "# Input folder names\n",
    "search_phrases = ['test_files']\n",
    "\n",
    "for folder in search_phrases:\n",
    "    all_tweets[folder] = merge_tweets(file_root=folder, skip=1,\n",
    "                                      file_start='filtered_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tweets the lazy way with large memory cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Use glob to search through all dates with specific hashtag\n",
    "# # then read the files and load the cumulitive result into a\n",
    "# # list to return\n",
    "\n",
    "# def merge_tweets(file_root, skip=100):\n",
    "#     ''' Function that compiles tweets from multiple files\n",
    "#         into a single list.  This may take a while.\n",
    "        \n",
    "#         file_root - Root directory to folder\n",
    "#         skip (int) - Number of files to skip over before storing\n",
    "#                      a tweet to memory.\n",
    "#         '''\n",
    "#     print(list(glob.iglob(file_root+'*')))\n",
    "#     tweet_files = list(glob.iglob(file_root+'*'))\n",
    "#     tweets = []\n",
    "#     for file in tweet_files:\n",
    "#         with open(file, 'r') as f:\n",
    "#             for i, line in enumerate(f.readlines()):\n",
    "#                     if i%skip == 0:\n",
    "#                         tweets.append(json.loads(line))\n",
    "#         print('finished importing file:', file)\n",
    "#     return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = merge_tweets(file_root='#nhl/', skip=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ''' Test cell '''\n",
    "    \n",
    "# for i, d in enumerate(data[::-1]):\n",
    "#     print(d['retweet_count'])\n",
    "#     print(d['favorite_count'])\n",
    "#     print(d['text'])\n",
    "#     print(d['user']['friends_count'])\n",
    "#     print(d['user']['followers_count'])\n",
    "#     print(d['user']['screen_name'])\n",
    "#     print(d['created_at'])\n",
    "#     print('')\n",
    "#     if i==0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_tweet_df(tweets):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['user'] = list(map(lambda tweet: tweet['user']['screen_name'], tweets))\n",
    "\n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], tweets))\n",
    "    \n",
    "    df['created_at'] = list(map(lambda tweet: tweet['created_at'], tweets))\n",
    "    \n",
    "    \n",
    "    \n",
    "#    df['location'] = list(map(lambda tweet: tweet['user']['location'], tweets))   \n",
    "#    df['country_code'] = list(map(lambda tweet: tweet['place']['country_code']\n",
    "#                                  if tweet['place'] != None else '', tweets))\n",
    "#    df['long'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][0]\n",
    "#                        if tweet['coordinates'] != None else 'NaN', tweets))   \n",
    "#    df['latt'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][1]\n",
    "#                        if tweet['coordinates'] != None else 'NaN', tweets))\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
